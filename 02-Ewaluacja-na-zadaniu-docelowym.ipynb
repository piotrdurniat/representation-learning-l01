{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8aa4606",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e6e11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67bc438",
   "metadata": {},
   "source": [
    "# Ewaluacja modeli\n",
    "W poprzednim zeszycie zbadaliśmy zachowanie różnych metod redukcji wymiarowości jako narzędzia do wizualizacji przestrzeni ukrytych (reprezentacji).\n",
    "\n",
    "Kolejnym krokiem będzie ewaluacja reprezentacji w konkretnych zadaniach, tak aby uzyskać liczbowe miary oceny jakości."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396ce35",
   "metadata": {},
   "source": [
    "# 3. Autokoder\n",
    "Jednym z najprosztych modeli uczenia reprezentacji jest poznany na wykładzie autokoder. Model taki składa się z dwóch modułów: **kodera** oraz **dekodera**. Zbudujemy model oparty na wielowarstwowych perceptronach i zastosujemy go w problemie uczenia reprezentacji dla rozpoznawania odręcznie pisanych cyfr (zbiór `digits` z pakietu scikit-learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb9f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X = X / 16.  # Każda cecha ma zakres wartości 0-16\n",
    "\n",
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ccfb29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Zbiór digits zawiera: {X.shape[0]:,} instancji\")\n",
    "print(f\"Każda cyfra jest opisana przez {X.shape[1]:,} cechy (pixele)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2754eb",
   "metadata": {},
   "source": [
    "## Zadanie 3.1 (0.5 pkt)\n",
    "Dokończ implementację autokodera. Zarówno koder jak i dekoder zaimplementuj jako dwu-warstwowe perceptrony z aktywacjami ReLU, tzn. (warstwa liniowa, aktywacja, warstwa liniowa, aktywacja). Pomiń ostatnią aktywację w module dekodera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421aa41",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c863efc8016ffd6c884f8946177e42d",
     "grade": true,
     "grade_id": "ae-implementation",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83c10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "    \n",
    "    \n",
    "def train(\n",
    "    model: Autoencoder, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    x: torch.Tensor,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x_rec = model(x)\n",
    "    loss = F.mse_loss(input=x_rec, target=x)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model: Autoencoder, x: torch.Tensor) -> float:\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_rec = model(x)\n",
    "        loss = F.mse_loss(input=x_rec, target=x)\n",
    "        \n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5266d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm    \n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "REPRESENTATION_DIM = 8\n",
    "\n",
    "\n",
    "ae = Autoencoder(feature_dim=X.shape[1], hidden_dim=REPRESENTATION_DIM)\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ae = ae.to(DEVICE)\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\"):\n",
    "    X_train = X_train.to(DEVICE)\n",
    "    X_test = X_test.to(DEVICE)\n",
    "    \n",
    "    train_loss = train(model=ae, optimizer=opt, x=X_train)\n",
    "    test_loss = test(model=ae, x=X_test)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(train_losses, label=\"Train\")\n",
    "ax.plot(test_losses, label=\"Test\")\n",
    "ax.set(title=\"Loss\", xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b383675",
   "metadata": {},
   "source": [
    "## Zadanie 3.2 (0.25 pkt)\n",
    "Zaimplemetuj metodę pozwalającą na ekstrakcję wektorów reprezentacji z modelu autokodera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ddaa6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44ef7590197bfd58bb0ccf957ae05495",
     "grade": true,
     "grade_id": "extract-representations",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575bd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_train = get_representations(model=ae, x=X_train).to(\"cpu\")\n",
    "z_test = get_representations(model=ae, x=X_test).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0daeadd-f8a6-41fb-9dc3-6a37b1d9d696",
   "metadata": {},
   "source": [
    "Zapisujemy reprezentacje na potrzeby zadania 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4433fb-3863-4986-99c1-7d05fd6f4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(z_train, \"z_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf361a",
   "metadata": {},
   "source": [
    "Wizualizujemy reprezentacje cyfr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfcd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "z2d_train = PCA(n_components=2).fit_transform(z_train.numpy())\n",
    "z2d_test = PCA(n_components=2).fit_transform(z_test.numpy())\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "for label in y_train.unique():\n",
    "    axs[0].scatter(z2d_train[y_train == label, 0], z2d_train[y_train == label, 1], label=label.item())\n",
    "    axs[1].scatter(z2d_test[y_test == label, 0], z2d_test[y_test == label, 1], label=label.item())\n",
    "\n",
    "axs[0].set(title=\"Train\")\n",
    "axs[1].set(title=\"Test\")\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102516b",
   "metadata": {},
   "source": [
    "# 4. Ewaluacja liniowa\n",
    "W przypadku ewaluacji liniowej zamrażamy wagi kodera i uczymy model liniowy na podanym zadaniu.\n",
    "\n",
    "W naszym przypadku reprezentacje już wyekstrahowaliśmy za pomocą metody `get_representations()`, zatem pozostaje nam etap wyuczenia modelu liniowego. Zastosujemy regresję logistyczną."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf64ae0",
   "metadata": {},
   "source": [
    "## Zadanie 4.1 (1.25 pkt)\n",
    "Zaimplementuj poniższą funkcję, która wyuczy model regresji logistycznej na danych `(z_train, y_train)`, a następnie obliczy wartość miary AUC na zbiorze treningowym oraz testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d84e4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec1af732a270453b7c1616a1732fc642",
     "grade": true,
     "grade_id": "linear-eval",
     "locked": false,
     "points": 1.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_linear(\n",
    "    z_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    z_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    ") -> tuple[float, float]:\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "auc_train, auc_test = evaluate_linear(\n",
    "    z_train=z_train,\n",
    "    y_train=y_train,\n",
    "    z_test=z_test,\n",
    "    y_test=y_test,\n",
    ")\n",
    "\n",
    "print(f\"Miara AUC na zbiorze treningowym: {auc_train * 100.:.2f} [%]\")\n",
    "print(f\"Miara AUC na zbiorze testowym: {auc_test * 100.:.2f} [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7e7fb",
   "metadata": {},
   "source": [
    "# 5. Ewaluacja douczania / z transferem wiedzy\n",
    "W tym scenariuszu koder jest uczony łącznie z modelem zadaniowym. Wykorzystamy wyuczony koder i połączymy go z pojedynczą warstwą liniową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9e318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS_FINETUNE = 100\n",
    "\n",
    "\n",
    "class FinetuneModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder: nn.Module, representation_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self._encoder = encoder\n",
    "        self._linear = nn.Linear(representation_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self._linear(self._encoder(x))\n",
    "        \n",
    "        \n",
    "finetune_model = FinetuneModel(\n",
    "    encoder=deepcopy(ae.encoder),\n",
    "    representation_dim=REPRESENTATION_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "opt = torch.optim.Adam(finetune_model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def train_finetune(\n",
    "    model: FinetuneModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = F.cross_entropy(input=y_pred, target=y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    auc = roc_auc_score(\n",
    "        y_true=y,\n",
    "        y_score=y_pred.detach().softmax(dim=-1).numpy(),\n",
    "        multi_class=\"ovr\",\n",
    "    )\n",
    "    \n",
    "    return loss.item(), auc\n",
    "\n",
    "\n",
    "def test_finetune(\n",
    "    model: FinetuneModel,\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(input=y_pred, target=y)\n",
    "        \n",
    "    auc = roc_auc_score(\n",
    "        y_true=y,\n",
    "        y_score=y_pred.softmax(dim=-1).numpy(),\n",
    "        multi_class=\"ovr\",\n",
    "    )\n",
    "    \n",
    "    return loss.item(), auc\n",
    "\n",
    "\n",
    "train_losses, train_aucs = [], []\n",
    "test_losses, test_aucs = [], []\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS_FINETUNE), desc=\"Epochs\"):\n",
    "    train_loss, train_auc = train_finetune(\n",
    "        model=finetune_model, \n",
    "        optimizer=opt,\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_aucs.append(train_auc)\n",
    "    \n",
    "    \n",
    "    test_loss, test_auc = test_finetune(\n",
    "        model=finetune_model, \n",
    "        x=X_test, \n",
    "        y=y_test,\n",
    "    )\n",
    "    test_losses.append(test_loss)\n",
    "    test_aucs.append(test_auc)\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(train_losses, label=\"Train\")\n",
    "axs[0].plot(test_losses, label=\"Test\")\n",
    "axs[0].set(title=\"Loss\", xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "\n",
    "axs[1].plot(train_aucs, label=\"Train\")\n",
    "axs[1].plot(test_aucs, label=\"Test\")\n",
    "axs[1].set(title=\"AUC\", xlabel=\"Epochs\", ylabel=\"AUC\")\n",
    "\n",
    "fig.legend()\n",
    "\n",
    "\n",
    "print(f\"Miara AUC na zbiorze treningowym: {train_aucs[-1] * 100.:.2f} [%]\")\n",
    "print(f\"Miara AUC na zbiorze testowym: {test_aucs[-1] * 100.:.2f} [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed931b0",
   "metadata": {},
   "source": [
    "## Zadanie 5.1 (0.25 pkt)\n",
    "Który ze scenariuszy ewaluacji pozwala uzyskać lepsze wyniki? Który byś wybrała/wybrał? Uzasadnij."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c082b6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f1d96da9f4c960fcf0199e5c9de6ff4",
     "grade": true,
     "grade_id": "eval-mode-comparison",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
